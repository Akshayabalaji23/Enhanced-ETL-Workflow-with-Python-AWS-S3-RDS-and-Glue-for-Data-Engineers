{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from botocore.exceptions import ClientError\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Setup logging\n",
    "log_file = r\"D:\\Data Engineering project 3\\Log files\\etl_process.txt\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# AWS credentials and configuration\n",
    "aws_access_key = \"AKIA2FXAECHO227L2TJB\"\n",
    "aws_secret_key = \"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\"\n",
    "region_name = \"us-east-1\"\n",
    "\n",
    "# S3 Bucket Names\n",
    "raw_data_bucket = \"newrawdatabucket23\"\n",
    "transformed_data_bucket = \"newtransformeddatabucket23\"\n",
    "\n",
    "# Local paths\n",
    "raw_data_path = \"D:\\\\Data Engineering project 3\"\n",
    "transformed_data_path = r\"D:\\Data Engineering project 3\\Transformed data\"\n",
    "\n",
    "# RDS details\n",
    "rds_host = \"database223.c27kq20wm2mf.us-east-1.rds.amazonaws.com\"\n",
    "db_user = \"admin\"\n",
    "db_password = \"nothingisworking\"\n",
    "db_name = \"database223\"\n",
    "rds_connection_string = f\"mysql+mysqlconnector://{db_user}:{db_password}@{rds_host}/{db_name}\"\n",
    "\n",
    "# S3 client\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key, region_name=region_name)\n",
    "\n",
    "# Function to upload files to S3\n",
    "def upload_file_to_s3(file_path, bucket_name):\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        s3_client.upload_file(file_path, bucket_name, file_name)\n",
    "        logging.info(f\"File {file_name} uploaded to S3 bucket {bucket_name}.\")\n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "\n",
    "# Function to download files from S3\n",
    "def download_file_from_s3(file_name, bucket_name, download_path):\n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, file_name, download_path)\n",
    "        logging.info(f\"File {file_name} downloaded from S3 bucket {bucket_name}.\")\n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Failed to download {file_name} from S3: {e}\")\n",
    "\n",
    "# Function to extract raw files from a ZIP and upload them to S3\n",
    "def extract_and_upload_raw_data():\n",
    "    for zip_file in os.listdir(raw_data_path):\n",
    "        if zip_file.endswith(\".zip\"):\n",
    "            zip_path = os.path.join(raw_data_path, zip_file)\n",
    "            with ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(raw_data_path)\n",
    "            for file_name in zip_ref.namelist():\n",
    "                file_path = os.path.join(raw_data_path, file_name)\n",
    "                upload_file_to_s3(file_path, raw_data_bucket)\n",
    "\n",
    "# Function to transform data\n",
    "def transform_data(input_file_path):\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Example transformations:\n",
    "    # Convert inches to meters\n",
    "    if 'inches' in df.columns:\n",
    "        df['inches'] = df['inches'] * 0.0254\n",
    "    \n",
    "    # Convert pounds to kilograms\n",
    "    if 'pounds' in df.columns:\n",
    "        df['pounds'] = df['pounds'] * 0.453592\n",
    "\n",
    "    # Clean and standardize data (example: fill missing values with 0)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Save transformed data to a new CSV file\n",
    "    transformed_file_path = os.path.join(transformed_data_path, \"transformed_data.csv\")\n",
    "    df.to_csv(transformed_file_path, index=False)\n",
    "    logging.info(f\"Data transformed and saved to {transformed_file_path}.\")\n",
    "    \n",
    "    return transformed_file_path\n",
    "\n",
    "# Function to upload transformed data to S3\n",
    "def upload_transformed_data_to_s3():\n",
    "    transformed_file_path = os.path.join(transformed_data_path, \"transformed_data.csv\")\n",
    "    upload_file_to_s3(transformed_file_path, transformed_data_bucket)\n",
    "\n",
    "# Function to load transformed data into RDS\n",
    "def load_data_to_rds(transformed_file_path):\n",
    "    # Create a connection to the RDS instance using SQLAlchemy\n",
    "    engine = create_engine(rds_connection_string)\n",
    "    \n",
    "    # Read the transformed data into a pandas DataFrame\n",
    "    df = pd.read_csv(transformed_file_path)\n",
    "    \n",
    "    # Load the data into the RDS table\n",
    "    df.to_sql('transformed_data_table', con=engine, index=False, if_exists='replace')\n",
    "    logging.info(f\"Data loaded into RDS table successfully.\")\n",
    "\n",
    "# Main ETL process\n",
    "def etl_process():\n",
    "    logging.info(\"ETL process started.\")\n",
    "    \n",
    "    # Step 1: Upload raw data to S3\n",
    "    extract_and_upload_raw_data()\n",
    "    \n",
    "    # Step 2: Download raw data from S3 for processing\n",
    "    for file_name in os.listdir(raw_data_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            download_file_from_s3(file_name, raw_data_bucket, os.path.join(raw_data_path, file_name))\n",
    "\n",
    "    # Step 3: Transform data\n",
    "    transformed_file_path = None\n",
    "    for file_name in os.listdir(raw_data_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(raw_data_path, file_name)\n",
    "            transformed_file_path = transform_data(file_path)\n",
    "\n",
    "    # Step 4: Upload transformed data to S3\n",
    "    upload_transformed_data_to_s3()\n",
    "\n",
    "    # Step 5: Load data into RDS\n",
    "    if transformed_file_path:\n",
    "        load_data_to_rds(transformed_file_path)\n",
    "\n",
    "    logging.info(\"ETL process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
